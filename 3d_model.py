# -*- coding: utf-8 -*-
"""Copy of 3D Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sFVdS_qfg-zGL4ahC8ehJDD4c5WP0O1X
"""

#!/usr/bin/env python3

import warnings
warnings.filterwarnings("ignore")

#from google.colab import drive
#drive.mount('/content/drive')

import sys
import os
#sys.path.append(os.path.abspath('/content/drive/My Drive/Colab Notebooks/Thesis/organized'))

import csv
import numpy as np
import matplotlib.pyplot as plt
import nibabel as nb
import keras
from keras import backend as K
from keras.models import Sequential, Input, Model
from keras.layers import Flatten, Input
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
import tensorflow as tf
import pandas as pd
from itertools import chain

from model import generator, unet, losses, analysis
from model.analysis import dice_train_statistics, dice_test_statistics, dice_final_statistics, subtype_statistics, predict_final, plot_predictions_train, plot_predictions_final
from model.losses import dice_coefficient, dice_loss, ComboLoss


base_dims = (251, 251, 58)
control_dims = (251, 251, 32)
batch_ctl = 4
continuing = False

root_dir = os.getcwd()
img_dir_3d = os.path.join(root_dir, "npy_img")
mask_dir_3d = os.path.join(root_dir, "npy_mask")
chkpts_dir = os.path.join(root_dir, "checkpoints")
holding_dir = os.path.join(chkpts_dir, "final")
train_csv_base = os.path.join(holding_dir, "train_files")
test_csv_base = os.path.join(holding_dir, "test_files")
log_dir = os.path.join(holding_dir, "logs")

diagnoses = pd.read_csv(os.path.join(root_dir, "hemorrhage_diagnosis_raw_ct.csv"))

fold = 0
epochs = 0
u_net = unet.build_UNet()

_, __, items = next(os.walk(img_dir_3d))
partition = int(0.2 * len(items))
np.random.shuffle(items)

if continuing: 
  _, checkpoints, __ = next(os.walk(holding_dir))
  log = pd.read_csv(os.path.join(log_dir, "logger" + str(len(checkpoints)-2) + ".csv"))
  u_net.compile(loss=ComboLoss,
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=[dice_coefficient, 'accuracy'])
  u_net.load_weights(os.path.join(holding_dir, "model" + str(len(checkpoints)-2)))
  epochs = len(log['epoch'])
  fold = len(checkpoints) - 2
else:
  for i in range(5):
    train_csv = train_csv_base + str(i) + '.csv'
    test_csv = test_csv_base + str(i) + '.csv'
    with open(test_csv,'w+') as csvfile:
      files = items[partition*i:partition*i+partition]
      wr = csv.writer(csvfile, dialect='excel')
      wr.writerows([files])


    with open(train_csv,'w+') as csvfile:
      files = items[0:partition*i]
      files += items[partition*i+partition:]
      wr = csv.writer(csvfile, dialect='excel')
      wr.writerows([files])

def train_multi(tries=4, n_epochs=8):

    saved = None
    saved_loss = 0

    for i in range(tries):
      u_net = unet.build_UNet()
      u_net.compile(loss=ComboLoss,
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=[dice_coefficient, 'accuracy'])
    
      current = u_net.fit(x=ones_generator, epochs=n_epochs, validation_data=test_generator, validation_freq=2, verbose=True, 
               max_queue_size=4, callbacks=[reduce_lr], steps_per_epoch=20)
      if(current.history['val_dice_coefficient'][-1] > saved_loss):
        saved_loss = current.history['val_dice_coefficient'][-1]
        saved = u_net
      print('-----------------------------------------------------------------------------------------------------')

    return saved

while(fold < 5):
  print(fold)

  train_csv = train_csv_base + str(fold) + '.csv'
  test_csv = test_csv_base + str(fold) + '.csv'
  
  with open(test_csv, 'r', newline='') as csvfile:
      test_items = []
      reader = csv.reader(csvfile, delimiter=',')
      for item in [j for sub in list(reader) for j in sub]:
         test_items.append(item)
      items[:partition] = test_items
      print(test_items)

  with open(train_csv, 'r', newline='') as csvfile:
      train_items = []
      reader = csv.reader(csvfile, delimiter=',')
      for item in [j for sub in list(reader) for j in sub]:
         train_items.append(item)
      items[partition:] = train_items
      print(train_items)

  train_generator = generator.DataGenerator(items=items, dim=control_dims, batch_size=batch_ctl, partition=partition, 
      img_dir=img_dir_3d, mask_dir=mask_dir_3d, undersampling=True, split=True, augment=True)
  test_generator = generator.DataGenerator(items=items, dim=control_dims, batch_size=2, partition=partition, 
      img_dir=img_dir_3d, mask_dir=mask_dir_3d, validation=True, undersampling=False, split=True, augment=False)
  ones_generator = generator.DataGenerator(items=items, dim=control_dims, batch_size=batch_ctl, partition=partition, 
      img_dir=img_dir_3d, mask_dir=mask_dir_3d, validation=False, undersampling=False, split=True, augment=True, ones_only=True)
  train_complete_generator = generator.DataGenerator(items=items, dim=control_dims, batch_size=batch_ctl, partition=partition, 
      img_dir=img_dir_3d, mask_dir=mask_dir_3d, undersampling=False, split=True, augment=False, complete=True)
  test_complete_generator = generator.DataGenerator(items=items, dim=control_dims, batch_size=batch_ctl, partition=partition, 
      img_dir=img_dir_3d, mask_dir=mask_dir_3d, validation=True, undersampling=False, split=True, augment=True, complete=True)
  final_generator = generator.DataGenerator(items=items, dim=control_dims, batch_size=batch_ctl, partition=partition, 
      img_dir=img_dir_3d, mask_dir=mask_dir_3d, validation=True, undersampling=False, split=False, augment=False, complete=True)
  
  reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=6)
  early_stopping = tf.keras.callbacks.EarlyStopping(monitor="loss", min_delta=0, patience=3,verbose=0, mode="min", baseline=None, 
                                                  restore_best_weights=False)

  if epochs < 8: 
        u_net = train_multi(4, 8)
        epochs = 8

  checkpoint_filepath = os.path.join(holding_dir, "model" + str(fold))
  file = open(os.path.join(log_dir, "logger" + str(fold) + ".csv"),'w+').close()
  logger = tf.keras.callbacks.CSVLogger(os.path.join(log_dir, "logger" + str(fold) + ".csv"), separator=",", append=True)
  model_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_dice_coefficient', mode='max', save_best_only=True)

  if epochs < 28: 
    u_net.fit(x=ones_generator, epochs=48-epochs, validation_data=test_generator, validation_freq=4, verbose=True, 
               max_queue_size=4, callbacks=[reduce_lr, model_checkpoint, logger])
    epochs = 48
  if epochs < 148: 
    u_net.fit(x=train_generator, epochs=148-epochs, validation_data=test_generator, validation_freq=4, verbose=True, 
               max_queue_size=4, callbacks=[reduce_lr, model_checkpoint, logger])

  dice_train_statistics(train_complete_generator, u_net)
  dice_test_statistics(test_complete_generator, u_net)
  dice_final_statistics(final_generator, u_net)
  subtype_statistics(final_generator, u_net, root_dir, img_dir_3d, mask_dir_3d)
    
  epochs = 0

  print("")

  fold += 1


